{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "urdkOl5FOLHP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_vy-FFpRcd8w",
        "outputId": "1dca9237-171f-48f8-c3fc-5db71d65d4e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lecture 1: Reading Image Data from Files"
      ],
      "metadata": {
        "id": "M9vNV9brVQSz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_shape = (300, 300)\n",
        "image_path = '/content/drive/MyDrive/garbage_classification'"
      ],
      "metadata": {
        "id": "9SHiaAoMOVCJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = tf.keras.utils.image_dataset_from_directory(image_path, shuffle=True, batch_size=32,\n",
        "                                                            image_size=image_shape, subset='training',\n",
        "                                                            validation_split=0.3, seed=42)\n",
        "# seed = what pattern the random shuffle follows\n",
        "# Must have same seeds, b/c the seed determines what the data is testing and validating\n",
        "\n",
        "validation_dataset = tf.keras.utils.image_dataset_from_directory(image_path, shuffle=True, batch_size=32,\n",
        "                                                                image_size=image_shape, subset='training',\n",
        "                                                                validation_split=0.3, seed=42)\n",
        "\n",
        "# This reduces the run time by reducing the data (only do this for practice not for real)\n",
        "train_batches = tf.data.experimental.cardinality(train_dataset)\n",
        "train_dataset = train_dataset.take(train_batches // 10) # // means for every fifth piece of data, for every 5 batches, take one of them (reduces to 20%)\n",
        "\n",
        "# Establish testing data\n",
        "validation_batches = tf.data.experimental.cardinality(validation_dataset)\n",
        "validation_dataset = validation_dataset.skip(train_batches // 5) # // skip every fifth one (gets 80% of original data)\n",
        "test_dataset = validation_dataset.take(validation_batches // 5)\n",
        "\n",
        "# We want to gather as many images as we can based on RAM amount before training. The amount we can gather at one time is limited by RAM.\n",
        "# AUTOTUNE helps us figure out how much RAM we have\n",
        "# buffer_size is just how many images we can import, which is determined by AUTOTUNE\n",
        "# Once one batch is through, we will go to the next batch, and then the next batch\n",
        "# Let hardware determine space for preloading images\n",
        "\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)\n",
        "validation_dataset = validation_dataset.prefetch(buffer_size=AUTOTUNE)\n",
        "test_dataset = test_dataset.prefetch(buffer_size=AUTOTUNE)"
      ],
      "metadata": {
        "id": "8_3huzzOORcv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebed098f-540e-45a1-e60d-8f38fbc8fd76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 15515 files belonging to 12 classes.\n",
            "Using 10861 files for training.\n",
            "Found 15515 files belonging to 12 classes.\n",
            "Using 10861 files for training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_final_shape = image_shape + (3,) # We need to add our channels\n",
        "num_classes = 12 # 12 classes\n",
        "print(image_final_shape)"
      ],
      "metadata": {
        "id": "pZN_xVjGR889",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cac3402e-439b-42a2-dc36-3f6940f14d02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(300, 300, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lecture 2: Implement the Convolutional Network"
      ],
      "metadata": {
        "id": "IopRMTaeVxAq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "My Notes:\n",
        "\n",
        "MaxPooling takes the \"footprint\", it reduces dimensions by taking a value from the quadrants (see slides)\n",
        "\n",
        "Global Average does the same thign as MaxPooling but takes the average"
      ],
      "metadata": {
        "id": "BIHU0EtAGsiK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nzEFyCi0NdOn"
      },
      "outputs": [],
      "source": [
        "### New Code Here! ###\n",
        "data_augmentation = tf.keras.Sequential([tf.keras.layers.Rescaling(1/255),\n",
        "                                         tf.keras.layers.RandomFlip('horizontal'),\n",
        "                                         tf.keras.layers.RandomRotation(0.2)])\n",
        "\n",
        "### End New Code ###\n",
        "\n",
        "prediction_layer = tf.keras.layers.Dense(12, activation='softmax')\n",
        "inputs = tf.keras.Input(shape=image_final_shape)\n",
        "### New Code Here! ###\n",
        "x = data_augmentation(inputs)\n",
        "x = tf.keras.layers.Conv2D(filters=32, kernel_size=(3,3))(x)\n",
        "x = tf.keras.layers.MaxPooling2D(pool_size=(2,2))(x) # Image is shrunk in half\n",
        "# The number in the dimension of a square pool_size is the denominator for how much image shrinks\n",
        "x = tf.keras.layers.Conv2D(filters=32, kernel_size=(3,3))(x)\n",
        "x = tf.keras.layers.MaxPooling2D(pool_size=(2,2))(x)\n",
        "x = tf.keras.layers.Conv2D(filters=32, kernel_size=(3,3))(x)\n",
        "x = tf.keras.layers.MaxPooling2D(pool_size=(2,2))(x)\n",
        "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "# x = tf.keras.layers.Flatten()(x)\n",
        "# x = tf.keras.layers.Dense(96)(x)\n",
        "\n",
        "### End New Code ###\n",
        "outputs = prediction_layer(x)\n",
        "model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "base_learning_rate = 0.001\n",
        "\n",
        "model_optimizer=tf.keras.optimizers.Adam(learning_rate=base_learning_rate)\n",
        "model_loss=tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "model_accuracy_metric=tf.keras.metrics.SparseCategoricalAccuracy(name='sparse_categorical_accuracy')\n",
        "\n",
        "model.compile(optimizer=model_optimizer, loss=model_loss,\n",
        "              metrics=[model_accuracy_metric])\n",
        "\n",
        "model_epochs = 10\n",
        "\n",
        "model_early_stopping = tf.keras.callbacks.EarlyStopping(patience=5,\n",
        "                                                       restore_best_weights=True,\n",
        "                                                       monitor='val_loss')\n",
        "\n",
        "model_history = model.fit(train_dataset, epochs=model_epochs,\n",
        "                         validation_data=validation_dataset,\n",
        "                         callbacks=[model_early_stopping])\n",
        "\n",
        "model_accuracy = model_history.history['sparse_categorical_accuracy']\n",
        "model_val_accuracy = model_history.history['val_sparse_categorical_accuracy']\n",
        "\n",
        "model_loss = model_history.history['loss']\n",
        "model_val_loss = model_history.history['val_loss']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.plot(model_accuracy, label='Training Accuracy')\n",
        "plt.plot(model_val_accuracy, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Training and Validation Accuracy')"
      ],
      "metadata": {
        "id": "NmtcWOxVOIWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(model_loss, label='Training Loss')\n",
        "plt.plot(model_val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.ylabel('Cross Entropy')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "iLYL2P5KSuxV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}