{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "urdkOl5FOLHP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_vy-FFpRcd8w",
        "outputId": "661fa29d-4d64-4eb8-f099-d7c63e707421"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lecture 1: Reading Image Data from Files"
      ],
      "metadata": {
        "id": "M9vNV9brVQSz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_shape = (300, 300)\n",
        "img_path='/content/drive/MyDrive/garbage_classification/'"
      ],
      "metadata": {
        "id": "9SHiaAoMOVCJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = tf.keras.utils.image_dataset_from_directory(img_path, shuffle=True,\n",
        "                                                            batch_size=32,\n",
        "                                                            image_size=image_shape,\n",
        "                                                            subset='training',\n",
        "                                                            validation_split=0.3,\n",
        "                                                            seed=1)\n",
        "\n",
        "validation_dataset = tf.keras.utils.image_dataset_from_directory(img_path,\n",
        "                                                                 shuffle=True,\n",
        "                                                                 batch_size=32,\n",
        "                                                                 image_size=image_shape,\n",
        "                                                                 subset='validation',\n",
        "                                                                 validation_split=0.3,\n",
        "                                                                 seed=1)\n",
        "\n",
        "# Shrink the training data to a size we can process during lecture.\n",
        "train_batches = tf.data.experimental.cardinality(train_dataset)\n",
        "train_dataset = train_dataset.take(train_batches // 10) # Comment out this line to use the full dataset\n",
        "\n",
        "# Take every 5th entry as the testing dataset.\n",
        "val_batches = tf.data.experimental.cardinality(validation_dataset)\n",
        "test_dataset = validation_dataset.take(val_batches // 5)\n",
        "validation_dataset = validation_dataset.skip(val_batches // 5)\n",
        "\n",
        "# Let the hardware determine how many pictures to load at a time.\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)\n",
        "validation_dataset = validation_dataset.prefetch(buffer_size=AUTOTUNE)\n",
        "test_dataset = test_dataset.prefetch(buffer_size=AUTOTUNE)"
      ],
      "metadata": {
        "id": "8_3huzzOORcv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f81d6319-b95c-41af-a001-cdfbf154035a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 15515 files belonging to 12 classes.\n",
            "Using 10861 files for training.\n",
            "Found 15515 files belonging to 12 classes.\n",
            "Using 4654 files for validation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_final_shape = image_shape + (3,) # Add the three colour channels to the image shape"
      ],
      "metadata": {
        "id": "pZN_xVjGR889"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lecture 2: Implement the Convolutional Network"
      ],
      "metadata": {
        "id": "IopRMTaeVxAq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nzEFyCi0NdOn"
      },
      "outputs": [],
      "source": [
        "data_augmentation = tf.keras.Sequential([tf.keras.layers.RandomFlip('horizontal'),\n",
        "                                         tf.keras.layers.RandomRotation(0.2)])\n",
        "global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
        "\n",
        "prediction_layer = tf.keras.layers.Dense(12, activation='softmax')\n",
        "inputs = tf.keras.Input(shape=image_final_shape)\n",
        "x = data_augmentation(inputs)\n",
        "x = tf.keras.layers.Conv2D(filters=32)(x)\n",
        "x = tf.keras.layers.MaxPooling2D(pool_size=(2,2))(x)\n",
        "x = tf.keras.layers.Conv2D(filters=32)(x)\n",
        "x = tf.keras.layers.MaxPooling2D(pool_size=(2,2))(x)\n",
        "x = tf.keras.layers.Conv2D(filters=32)(x)\n",
        "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "x = tf.keras.layers.Dropout(0.2)(x)\n",
        "outputs = prediction_layer(x)\n",
        "model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "base_learning_rate = 0.001\n",
        "\n",
        "model_optimizer=tf.keras.optimizers.Adam(learning_rate=base_learning_rate)\n",
        "model_loss=tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "model_accuracy_metric=tf.keras.metrics.SparseCategoricalAccuracy(name='sparse_categorical_accuracy')\n",
        "\n",
        "model.compile(optimizer=model_optimizer, loss=model_loss,\n",
        "              metrics=[model_accuracy_metric])\n",
        "\n",
        "model_epochs = 10\n",
        "model_loss, model_accuracy = model.evaluate(validation_dataset)\n",
        "\n",
        "model_early_stopping = tf.keras.callbacks.EarlyStopping(patience=5,\n",
        "                                                       restore_best_weights=True,\n",
        "                                                       monitor='val_loss')\n",
        "\n",
        "model_history = model.fit(train_dataset, epochs=model_epochs,\n",
        "                         validation_data=validation_dataset,\n",
        "                         callbacks=[model_early_stopping])\n",
        "\n",
        "fine_tune_epochs = 10\n",
        "\n",
        "model_accuracy = model_history.history['sparse_categorical_accuracy']\n",
        "model_val_accuracy = model_history.history['val_sparse_categorical_accuracy']\n",
        "\n",
        "model_loss = model_history.history['loss']\n",
        "model_val_loss = model_history.history['val_loss']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.plot(model_accuracy, label='Training Accuracy')\n",
        "plt.plot(model_val_accuracy, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Training and Validation Accuracy')"
      ],
      "metadata": {
        "id": "NmtcWOxVOIWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(model_loss, label='Training Loss')\n",
        "plt.plot(model_val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.ylabel('Cross Entropy')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "iLYL2P5KSuxV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}